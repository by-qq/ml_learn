{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "\n",
    "不能，numpy主要用于数据预处理，科学计算、小规模数值计算；tensorflow用于机器学习模型训练，需要硬件加速计算\n",
    "\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "\n",
    "结果相同但是数据类型不同\n",
    "\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "\n",
    "普通函数：用于简单的数学运算，无状态和配置参数\n",
    "Loss子类：保存状态、配置参数、复杂逻辑、序列化需求\n",
    "\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "\n",
    "普通函数：简单指标，无状态需求\n",
    "继承子类：需要维护状态，复杂指标计算\n",
    "\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "\n",
    "自定义层：创建可重用的基础组件\n",
    "自定义模型：定义完整的模型架构、训练逻辑、自定义方法\n",
    "\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "\n",
    "不同任务的不同权重和损失、需要动态调整的，自定义优化器的\n",
    "\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "\n",
    "在eager模式下：可以正常运行所有Python代码\n",
    "在graph模式下，必须使用TF函数转换\n",
    "\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "\n",
    "使用python原生类型，python的打印语句，文件操作\n",
    "修改外部状态\n",
    "使用可变数据类型\n",
    "\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n",
    "\n",
    "动态调整层数、有分支语句的时候、有依赖的形状的时候\n",
    "\n",
    "定义继承自tf.keras.Model模型的类在其中编写build\\call函数\n",
    "\n",
    "因为静态图更好优化，性能更好，方便调试和理解，更用于保存"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T00:56:58.634754Z",
     "start_time": "2025-09-15T00:56:58.609816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,epsilon= 1e-3,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight( name=\"alpha\",shape=input_shape[-1:], initializer='ones', dtype=tf.float32)\n",
    "        self.beta = self.add_weight( name=\"beta\",shape=input_shape[-1:], initializer='zeros', dtype= tf.float32)\n",
    "        super().build(input_shape)  # 这种写法更简洁\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        # 计算每个实例特征的均值和标准差\n",
    "        # tf.nn.moments(inputs, axes=-1,keepdims=True) 返回同一实例的均值和方差\n",
    "        self.mu,self.sigma = tf.nn.moments(inputs, axes=-1,keepdims=True)\n",
    "\n",
    "        return self.alpha * (inputs - self.mu)/(self.sigma+self.epsilon) + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config,\"epsilon\":self.epsilon}"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T00:58:45.802161Z",
     "start_time": "2025-09-15T00:58:45.735337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试函数\n",
    "def test_layer_normalization():\n",
    "    print(\"测试层归一化实现...\")\n",
    "\n",
    "    # 创建测试数据\n",
    "    batch_size, seq_length, hidden_size = 4, 6, 8\n",
    "    test_input = tf.random.normal([batch_size, seq_length, hidden_size])\n",
    "    print(f\"输入形状: {test_input.shape}\")\n",
    "\n",
    "    # 使用自定义层归一化\n",
    "    custom_ln = MyDense(epsilon=1e-3)\n",
    "    custom_output = custom_ln(test_input)\n",
    "    print(f\"自定义层输出形状: {custom_output.shape}\")\n",
    "\n",
    "    # 使用标准层归一化\n",
    "    standard_ln = tf.keras.layers.LayerNormalization(epsilon=1e-3)\n",
    "    standard_output = standard_ln(test_input)\n",
    "    print(f\"标准层输出形状: {standard_output.shape}\")\n",
    "\n",
    "    # 计算差异\n",
    "    absolute_diff = tf.abs(custom_output - standard_output)\n",
    "    max_diff = tf.reduce_max(absolute_diff)\n",
    "    mean_diff = tf.reduce_mean(absolute_diff)\n",
    "\n",
    "    print(f\"最大绝对差异: {max_diff.numpy():.6e}\")\n",
    "    print(f\"平均绝对差异: {mean_diff.numpy():.6e}\")\n",
    "\n",
    "    # 检查是否几乎相同\n",
    "    tolerance = 1e-6\n",
    "    if max_diff < tolerance:\n",
    "        print(\"✓ 测试通过！自定义层与标准层输出几乎相同\")\n",
    "    else:\n",
    "        print(\"✗ 测试失败！输出存在明显差异\")\n",
    "\n",
    "    return custom_output, standard_output, max_diff, mean_diff\n",
    "\n",
    "\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 基本功能测试\n",
    "    custom_out, standard_out, max_diff, mean_diff = test_layer_normalization()\n"
   ],
   "id": "9423399c6c670494",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试层归一化实现...\n",
      "输入形状: (4, 6, 8)\n",
      "自定义层输出形状: (4, 6, 8)\n",
      "标准层输出形状: (4, 6, 8)\n",
      "最大绝对差异: 1.929823e+00\n",
      "平均绝对差异: 3.381391e-01\n",
      "✗ 测试失败！输出存在明显差异\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "7a0accaa76bd106d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T02:31:51.018761Z",
     "start_time": "2025-09-15T02:31:50.992334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义循环训练模型,自定义块和自定义模型\n",
    "# 先自定义循环块\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_layers,n_neurons=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = [tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        z = inputs\n",
    "        for layer in self.hidden1:\n",
    "            z = layer(z)\n",
    "\n",
    "        return inputs + z\n",
    "\n",
    "# 自定义模型\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self,output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 第一个全连接层，将2维输入扩展到30维\n",
    "        self.input_layer = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "        self.block = ResidualBlock(2)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation=\"softmax\")\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        # 首先将2维输入扩展到30维\n",
    "        z = self.input_layer(inputs)\n",
    "        z = self.block(inputs)\n",
    "        return self.out(z)"
   ],
   "id": "b07d40dc2c0627fa",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T02:30:59.669790Z",
     "start_time": "2025-09-15T02:30:59.629713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris.target\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=42)"
   ],
   "id": "4370ca2c5688af11",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T02:31:54.902689Z",
     "start_time": "2025-09-15T02:31:53.136914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ResidualRegressor(3)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # 适用于整数标签\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=3, validation_split=0.1, verbose=1)"
   ],
   "id": "5be77200660c4baf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor_13/dense_62/kernel:0', 'residual_regressor_13/dense_62/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor_13/dense_62/kernel:0', 'residual_regressor_13/dense_62/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor_13/dense_62/kernel:0', 'residual_regressor_13/dense_62/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['residual_regressor_13/dense_62/kernel:0', 'residual_regressor_13/dense_62/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "4/4 [==============================] - 2s 86ms/step - loss: 1.1620 - accuracy: 0.5741 - val_loss: 1.1991 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.1542 - accuracy: 0.6019 - val_loss: 1.1922 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.1463 - accuracy: 0.6019 - val_loss: 1.1853 - val_accuracy: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23271ed0370>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T02:32:07.836020Z",
     "start_time": "2025-09-15T02:32:07.696392Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(X_test, y_test)",
   "id": "24c9508c2f38d62e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1884 - accuracy: 0.5667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1883660554885864, 0.5666666626930237]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "983da7a065080f55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
